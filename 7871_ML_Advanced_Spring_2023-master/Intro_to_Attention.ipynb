{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention: Motivation\n",
    "\n",
    "The models that we studied for processing input sequences differed from models for non-sequence inputs\n",
    "- **memory** (latent state) required for processing sequences\n",
    "    - because sequence length is unbounded\n",
    "    - finite representation of unbounded length input sequence\n",
    "    - output at step $\\tt$ fed as input to step $\\tt+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The use of latent state/memory evolved over the models we studied\n",
    "- RNN\n",
    "    - latent state encodes\n",
    "        - input representation\n",
    "        - \"control\" state\n",
    "            - guiding how the model processes the data: state transitions\n",
    "- LSTM\n",
    "    - latent state partitioned into\n",
    "        - Short Term memory: control state\n",
    "        - Long Term memory\n",
    "        \n",
    "Both these models processed the input sequence **once**\n",
    "- so input-specific representation needs to be part of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will introduce a mechanism called *Attention*\n",
    "- that allows the input sequence to be *re-visited* at each time step\n",
    "- cleaner separation between control memory and input memory\n",
    "\n",
    "Let's revisit the Encoder-Decoder architecture\n",
    "\n",
    "The Encoder\n",
    "- Acts on input sequence $[\\x_{(1)} \\dots \\x_{(\\bar{T})}]$\n",
    "- Producing a sequence of latent states $[ \\bar{\\h}_{(1)}, \\dots, \\bar{\\h}_{(\\bar{T})} ]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder\n",
    "- Acts on the *final* Encoder latent state $\\bar{\\h}_{(\\bar{T})}$\n",
    "- Producing a sequence of outputs $[ \\hat{\\y}_{(1)}, \\dots, \\hat{\\y}_{(T)} ]$\n",
    "- Often feeding step $\\tt$ output $\\hat{\\y}_\\tp$ as Encoder input at step $(\\tt+1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder.png\"</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The following diagram is a condensed depiction of the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: training (teacher forcing) + inference: No attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The topic of \"Attention\" will focus on the part of the Decoder diagram above\n",
    "that transforms Decoder latent state (or short term memory) $\\h_\\tp$ to output $\\hat{\\y}_\\tp$.\n",
    "\n",
    "It is a Neural Network implementing a function \n",
    "$$\\hat\\y_\\tp = D(\\h_\\tp)$$\n",
    "mapping\n",
    "- the Decoder short term memory $\\h_\\tp$ \n",
    "- to next output $\\hat\\y_\\tp$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder: No attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_no_attention.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an illustrative example: consider the task of Question Answering.\n",
    "- a sequence to sequence task (thus, ideal for an Encoder-Decoder architecture)\n",
    "- where the input sequence $\\x_{[1:\\bar T]}$ is the pair consisting of\n",
    "    - a paragraph called the *context*\n",
    "    - a question that references the context\n",
    "- the target/label (i.e., desired output sequence) is $\\y_{[1:T]}$\n",
    "    - is text that \"answers\" the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\x = \\;\n",
    "\\begin{Bmatrix}\\\\\n",
    "\\text{Context:} & \\text{The FRE Dept offers many Spring classes.  The students are great. ...} \\\\\n",
    "& \\vdots \\\\\n",
    "& \\text{Professor Perry taught them Machine Learning. The students ...}, \\\\\n",
    "& \\vdots \\\\\n",
    "& \\text{Professor Blecherman led a class in ...} \\\\\n",
    "& \\vdots \\\\\n",
    "\\text{Question:} & \\text{What did Professor Perry do ?} \\\\\n",
    "\\end{Bmatrix}\n",
    "$\n",
    "<br><br><br>\n",
    "$\n",
    "\\y = \\;\n",
    "\\begin{array} \\\\\n",
    "\\text{Answer:} & \\text{He taught them Machine Learning}\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose the Decoder has already output \n",
    "$$\\hat\\y_{([1:3])} = \\text{He taught them}$$\n",
    "\n",
    "The remainder of the desired output sequence is\n",
    "$$\n",
    "\\hat\\y_{([4:5])} = \\text{Machine Learning}$$\n",
    "\n",
    "How is  possible for the Neural Network implementing $D$ to produce\n",
    "$$\\hat\\y_{(4)} = D( \\h_{(4)} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that $D$ is conditioned on the single input $\\h_\\tp$.\n",
    "\n",
    "Thus, in order for $D( \\h_{(4)} )$ to be equal to \"Machine\"\n",
    "- this information must somehow be encoded in $\\h_{(4)}$\n",
    "\n",
    "But how did it get there ?\n",
    "\n",
    "It must have been encoded in $\\bar \\h_{(\\bar T)}$\n",
    "- the \"summary\" of $\\x_{([1:\\bar T])}$ passed by the Encoder to the Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that Decoder ouput $\\bar{\\h}_{(\\bar{\\tt})}$ is a fixed length encoding of the input prefix $\\x_{(1)}, \\ldots, \\x_{(\\bar{\\tt})} $.\n",
    "\n",
    "For example:\n",
    "$$\\x_{0}, \\ldots , \\x_{\\bar T} = \\text{Machine, learning, is, easy, not, hard}$$\n",
    "\n",
    "<br>\n",
    "\\begin{array} \\\\\n",
    "\\bar\\h_{(0)} & = & \\text{summary}( [ \\text{Machine} ]) \\\\\n",
    "\\bar\\h_{(1)} & = & \\text{summary}( [ \\text{Machine, Learning} ]) \\\\\n",
    "\\vdots \\\\\n",
    "\\bar\\h_{\\bar{\\tt}} & = & \\text{summary}( [ \\x_{(0)}, \\ldots \\x_{(\\bar{\\tt})}] ) \\\\\n",
    "\\vdots \\\\\n",
    "\\bar\\h_{(5)} & = & \\text{summary}( [ \\text{Machine, Learning, is, easy, not, hard} ]) \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order for the concept \"Machine Learning\" to have been encoded in $\\bar \\h_{(\\bar T)}$\n",
    "- it must be present in all Encoder latent states \n",
    "$$\\bar \\h_{({\\bar \\tt}')} \\text{ for } \\tt' \\ge p$$\n",
    "where $p$ is the index of \"Machine Learning\" in the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To summarize\n",
    "- because $D$ is conditioned *only* on Decoder state $\\h_\\tp$\n",
    "- all \"facts\" from the context must be transfered from Encoder to Decoder\n",
    "- through final Encoder state $\\bar \\h_{(\\bar T)}$\n",
    "- which in turn was encoded in all Encoder states $\\bar \\h_{({\\bar \\tt}')} \\text{ for } \\tt' \\ge p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The choice of conditioning $D$ only on $\\h_\\tp$ is burdensome for both the Encoder and Decoder.\n",
    "\n",
    "some of the weights of each must be devoted to \n",
    "- \"control\"\n",
    "    - how to record facts (Encoder)/produce output (Decoder) in the abstract (i.e., for any context/question)\n",
    "- concrete facts of the particular context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attend to what's important\n",
    "\n",
    "What if we changed $D$ so it was conditioned on both $\\h_\\tp$ **and** $\\bar \\h_{([1:\\bar T])}$\n",
    "- $\\bar \\h_{([1:\\bar T])}$ enables the Decoder to refer back to input $\\x_{([1:\\bar T])}$ at *every output position* $\\tt$\n",
    "- $\\h_\\tp$ no longer has to encode the \"facts\"\n",
    "    - weights can be devoted to \"control\"\n",
    "- $\\bar \\h_{(\\bar T)}$ is *no longer the bottleneck* through which facts flow from Encoder to Decoder\n",
    "\n",
    "The version of $D$ with Attention looks something like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder: Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_attention.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At output position $\\tt$\n",
    "we enable the Decoder to focus on (*attend to*)\n",
    "- the position $\\bar \\tt$ of the input\n",
    "- that is *relevant* for producing $\\hat \\y_\\tp$\n",
    "\n",
    "This seems very natural to a human\n",
    "- rather than memorizing details\n",
    "- we refer back to the context\n",
    "- focusing of only the part that is immediately needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The discussion of the **implementation** of Attention will be deferred to a\n",
    "later module [Attention lookup](Attention_Lookup.ipynb).\n",
    "\n",
    "For now, think of the \"Choose\" box as a Context Sensitive Memory (as described in the module on [Neural Programming](Neural_Programming.ipynb#Soft-Lookup))\n",
    "- Like a Python `dict`\n",
    "    - Collection of key/value pairs: $\\langle \\bar\\h_{(\\bar \\tt)}, \\bar\\h_{(\\bar \\tt)} \\rangle$\n",
    "    - Key is equal to value; they are latent states of the Encoder\n",
    "- But with *soft* lookup\n",
    "    - The current Decoder state $\\h_\\tp$ is presented to the CSM \n",
    "        - Called the *query*\n",
    "        - Is matched across each key of the dict (i.e., a latent state $\\bar \\h_{(\\bar \\tt)}$)\n",
    "    - The CSM returns an approximate match of the query to a *key* of the `dict`\n",
    "        - The distance between the query and each key in the CSM is computed\n",
    "        - The Soft Lookup returns a *weighted* (by inverse distance) sum of the *values* in the CSM `dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualizing Attention\n",
    "\n",
    "We can illustrate the behavior of Neural Networks that have been augmented with Attention through diagrams.\n",
    "- at a particular output position $\\tt$\n",
    "- we can display the amount of \"attention\"\n",
    "- that each position in the input receives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Attention can be used to create a Context Sensitive Encoding of words\n",
    "- The meaning of a word may change depending on the rest of the sentence\n",
    "\n",
    "We can illustrate this with an example: how the meaning of the word \"it\" changes\n",
    "- The thickness of the blue line indicates the attention weight that is given in processing the word \"it\".\n",
    "\n",
    "<img src=https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Much of the recent advances in NLP may be attributed to these improved, context sensitive embeddings.\n",
    "\n",
    "We note that simple Word Embeddings\n",
    "- also capture \"meaning\"\n",
    "- but are *not* sensitive to context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Entailment: Does the \"hypothesis\" logically follow from the \"premise\"**\n",
    "\n",
    "<br>\n",
    "<center><strong>Attention: Entailment</strong></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_visualization_Entailment.png\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Does the Premise logically entail the Hypothesis.</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/1509.06664.pdf#page=6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Date normalization example**\n",
    "- Source: Dates in free-form: \"Saturday 09 May 2018\"\n",
    "- Target: Dates in normalized form: \"2018-05-09\"\n",
    "\n",
    "[link](https://github.com/datalogue/keras-attention#example-visualizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Image captioning example**\n",
    "- Source: Image\n",
    "- Target: Caption: \"A woman is throwing a **frisbee** in a park.\"\n",
    "- Attending over *pixels* **not** sequence\n",
    "\n",
    "<center><strong>Visual attention</strong></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/shat_-002-027.jpg\"></td>\n",
    "        <td><img src=\"images/shat_-002-028.jpg\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=2><center>A woman is throwing a <strong>frisbee</strong> in a park.</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "Attribution: https://arxiv.org/pdf/1502.03044.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ## Benefits of Attention\n",
    "\n",
    "Continuing with our hypothetical Question Answering task\n",
    "- Attention allows the Decoder to focus on \"control\"\n",
    "- By allowing it to access \"facts\" when needed\n",
    "\n",
    "We don't **know** if the following is actually what happens, but let's imagine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Perhaps, after seeing many such examples, the Decoder \"learns\" a pattern for answering questions of the type\n",
    "\n",
    "> What did Professor `<PROPER NOUN>` teach in the Spring ?\n",
    "\n",
    "Output Pattern:\n",
    "```\n",
    "<PRONOUN> <VERB> <INIDRECT OBJECT> <OBJECT>\n",
    "```\n",
    "\n",
    "where `<PRONOUN>, <VERB>`, etc. are *pattern place-holders*\n",
    "\n",
    "So the \"control\" of the Decoder\n",
    "- needs to output each position of the output pattern\n",
    "- binding concrete values to each place-holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And perhaps the Encoder \"learns\" to create the bindings of concrete values to place-holders\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Answering questions using Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_example_1.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then the control state $\\h_\\tp$ for the Decoder could use Attention\n",
    "to **attend to** the binding for the next place-holder in the output pattern.\n",
    "- Following the pattern it learned\n",
    "- Issuing a \"query\" to lookup the concrete value bound to a place-holder (the \"key\")\n",
    "    - for each element of the pattern\n",
    "    \n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Answering questions using Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_example_2.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Have we seen this before ?\n",
    "\n",
    "If you recall the architecture of the LSTM\n",
    "- *short term* (control) memory \n",
    "- was separated from *long term* memory\n",
    "- elements of long term memory are moved to short term memory *as needed*\n",
    "\n",
    "This is partly similar to the advantages of Attention.\n",
    "\n",
    "But\n",
    "- all factual information from input $\\x$ has to flow through the bottleneck $\\bar \\h_{(\\bar T)}$ of the Encoder output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Self-attention\n",
    "\n",
    "We have illustrated the benefit of enabling the Decoder to attend to the Encoder.\n",
    "\n",
    "This form of attention is called *Cross Attention*.\n",
    "\n",
    "But we can further simplify the Decoder control by enabling it, when generating $\\hat \\y_\\tp$\n",
    "- to attend to all previously generated outputs $\\hat \\y_{([1:\\tt-1])}$\n",
    "\n",
    "This form of attention if called *Self Attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, suppose the Decoder is generating a long sentence\n",
    "- in many languages, there needs to be agreement between the gender/plurality of a subject and verbs\n",
    "- Self attention enables the Decoder to refer back to the previously generated subject of the sentence\n",
    "- when generating the verb for each subsequent output position\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is common in an Encoder-Decoder architecture to have both\n",
    "- Cross Attention from Decoder to Encoder\n",
    "- Self Attention from Decoder to Decoder\n",
    "\n",
    "We will see both forms used in the Transformer.\n",
    "\n",
    "These mechanisms are attending to different sequences (Encoder states or Decoder outputs).\n",
    "\n",
    "We will henceforth use the term *sequence being attended to* as a general term\n",
    "- instead of specifically referring to the part of the network that produced it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Masked attention\n",
    "\n",
    "As presented, the Attention mechanism can refer to an entire sequence\n",
    "- e.g., the sequence of Encoder latent states\n",
    "\n",
    "It is sometimes desirable to *limit* what may be attended to.\n",
    "\n",
    "For example, consider a decision at time $\\tt$ that may depend *only on the past*\n",
    "- positions $\\tt' \\lt \\tt$\n",
    "- for example, a trading decision at time $\\tt$ may depend only on *prior* information\n",
    "    - typical of sequences that are timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Restricting attention to the past is called *Causal Attention*.\n",
    "- the next output depends only on things that could have caused it (the past), not the future\n",
    "\n",
    "There is a mechanism to restrict what may be attended to in a general way\n",
    "- create a \"mask\"\n",
    "- a bit vector for each position of the sequence being attended to\n",
    "- such that attention is limited to positions where the mask element if True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called *Masked Attention*.\n",
    "\n",
    "It is frequently used to enable a Decoder, when predicting output $\\hat \\y_\\tp$\n",
    "- to attend to **previously** generate outputs $\\hat \\y_{[1:\\tt-1])}$\n",
    "- but not **future** outputs $\\hat\\y_{(\\tt')}$ for $\\tt' \\ge \\tt$\n",
    "\n",
    "When used in this manner, we refer to the behavior as *Masked Self Attention*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**\n",
    "\n",
    "You may wonder how it is even practically possible for a Decoder to refer to the future.\n",
    "\n",
    "When using *Teacher Forcing* for **training**\n",
    "- the Decoder does not use the *predicted* target sequence $\\hat \\y_{(1:T)}$\n",
    "- the Decoder uses the *actual* target sequence $\\y_{(1:T)}$\n",
    "    - hence, \"future\" positions $\\tt' \\ge \\tt$ are available\n",
    "- this prevents a single mis-prediction at position $\\tt$ from cascading and ruining all future output\n",
    "    - facilitates training\n",
    "- at inference time: the Decoder works on the *predicted* Target sequence.\n",
    "\n",
    "In the diagram below, we illustrate (lower right) how the Decoder input changes between Training and Test/Inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: training (teacher forcing) + inference: No attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-head attention: two heads are better than one\n",
    "\n",
    "Perhaps when generating the output for position $\\tt$ of the output sequence\n",
    "- we need to attend to *more than one* position of the sequence being attended to\n",
    "    - need to know both gender and plurality of subject\n",
    "- that is: we want an Attention layer to output multiple items.\n",
    "\n",
    "We can attend to $n$ positions\n",
    "- by creating $n$ separate Attention mechanisms\n",
    "- each one called a *head*\n",
    "\n",
    "This behavior is referred to as *Multi-head attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This type of behavior is common to many layer types in a Neural Network\n",
    "- a Dense layer $l$ may produce a vector $\\y_\\llp$ where $n_\\llp \\gt 1$\n",
    "- a Convolutional layer $l$ may produce outputs (for each spatial location) for many channels\n",
    "\n",
    "We have referred to this as layer $\\ll$ producing $n_\\llp$ *features*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It would be natural for an Attention layer to output many \"features\" to enable attention to many positions.\n",
    "\n",
    "In practice, this is sometimes (always ?) not done\n",
    "- Model architectures (e.g., the Transformer) are simplified when the inputs/outputs of each sub-component\n",
    "- have the same length\n",
    "- often denoted as $d$ of $d_\\text{model}$ in the Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When a Transformer needs to attend to $n$ positions\n",
    "- it uses $n$ Attention heads\n",
    "- each outputting a vector of length $\\frac{d}{n}$\n",
    "- which are concatenated together to produce a single output of length $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we have $n$ heads\n",
    "- Rather than having one Attention head operating on vectors of length $d$\n",
    "    - producing an output of length $d$ (weighted sum of values in the CSM)\n",
    "- We create $n$ Attention heads operating on vectors (keys, values, queries) of length $d \\over n$.\n",
    "    - Output of these smaller heads are values, and hence also of length $d \\over n$\n",
    "- The final output concatenates these $n$ outputs into a single output of length $d$\n",
    "    - identical in length to the single head\n",
    "- we project each of these length $d$ vector into vectors of length $d \\over n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The picture shows $n$ Attention heads.\n",
    "\n",
    "Note that each head is working on vectors of length $\\frac{d}{n}$ rather than\n",
    "original dimensions $d$.\n",
    "- variables with superscript $(j)$ are of fractional length\n",
    "\n",
    "Details are deferred to the module [Attention lookup](Attention_Lookup.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each head $j$ uniquely transforms the query $\\h_\\tp$ and the key/value pairs $\\bar{\\h}_{(1)} \\ldots \\bar{\\h}_{(\\bar{T})}$ being queried.\n",
    "- into $\\h^{(j)}_\\tp$ and the key/value pairs $\\bar{\\h}^{(j)}_{(1)} \\ldots \\bar{\\h}^{(j)}_{(\\bar{T})}$\n",
    "- Such that each head attends to a separate item\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder Multi-head Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Multihead_attention.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformers\n",
    "\n",
    "There is a new model (the Transformer) that processes sequences much faster than RNN's.\n",
    "\n",
    "It is an Encoder/Decoder architecture that uses multiple forms of Attention\n",
    "- Self Attention in the Encoder\n",
    "    - to tell the Encoder the relevant parts of the input sequence $\\x$ to attend to\n",
    "- Decoder/Encoder attention\n",
    "    - to tell the Decoder which Encoder state $\\bar{\\h}_{(\\tt')}$ to attend to when outputting $\\y_\\tp$\n",
    "- Masked Self-Attention in the Decoder\n",
    "    - to prevent the Decoder from looking ahead into inputs that have not yet been generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We recognized that the Decoder function responsible for generating Decoder output $\\hat{\\y}_\\tp$\n",
    "$$\n",
    "\\hat{\\y}_\\tp = D( \\h_\\tp; \\mathbf{s})\n",
    "$$\n",
    "\n",
    "was quite rigid when it ignored argument $\\mathbf{s}$.\n",
    "\n",
    "This rigidity forced Decoder latent state $\\h_\\tp$ to assume the additional responsibility of including Encoder context.\n",
    "\n",
    "Attention was presented as a way to obtain Encoder context through argument $\\mathbf{s}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
