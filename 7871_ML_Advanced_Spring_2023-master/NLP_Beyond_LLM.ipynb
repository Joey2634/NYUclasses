{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-train, prompt, predict \n",
    "\n",
    "We have presented the \"Unsupervised Pre-Training + Supervised Fine-Tuning\" paradigm.\n",
    "- Adapt a LM to a new task with further training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have also introduced *In Context Learning*\n",
    "- Adapt a LM to a new task by providing demonstrations of the new Target task behavior\n",
    "- via examples in the prompt\n",
    "\n",
    "Note that the exemplars are given at *inference* time **not** training time\n",
    "- the model's weights are **not updated**\n",
    "- the exemplars only condition the model into generating specific output\n",
    "\n",
    "This paradigm has been called [\"Pre-train, Prompt, Predict\"](https://arxiv.org/pdf/2107.13586.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, we can describe Translation between languages with the following context\n",
    "\n",
    "    Translate English to French\n",
    "    \n",
    "    sea otter =>  loutre de mer\n",
    "    \n",
    "    peppermint => menthe poivree\n",
    "    \n",
    "    plush giraffe => girafe peluche\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *exemplars* (pre-prompt) consists of\n",
    "- an initial string describing the task: \"Translate English to French\"\n",
    "- a number of examples\n",
    "    - English input, French output, Separated by a `=>`\n",
    "    \n",
    "The expectation is that when the user presents the prompt\n",
    "\n",
    "         cheese => \n",
    "         \n",
    "the model will respond with the French translation of `cheese`.\n",
    "- the \"next words\" predicted by the Language Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More formally: \n",
    "- Let $C$ (\"context\") denote the pre-prompt.\n",
    "- Let $\\x$ denote the \"query\" (e.g., `cheese =>`)\n",
    "\n",
    "The unconditional Language Modeling objective\n",
    "$$\n",
    "\\pr{\\y | \\x}\n",
    "$$\n",
    "is to create the sequence $\\y$ that follows the sequence of prompt $\\x$.\n",
    "\n",
    "Here, the pre-prompt conditions the model's objective\n",
    "$$\n",
    "\\pr{\\y | \\x,  C}\n",
    "$$\n",
    "to create the sequence $\\y$ that follows from the exemplars $C$ and prompt $\\x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To turn this into a Language Modeling task using the Universal API\n",
    "- we need to represent the exemplars and the prompt \n",
    "as a sequence.\n",
    "\n",
    "We create the sequence $\\dot \\x$\n",
    "by concatenating\n",
    "\n",
    "- some number $k$ of exemplars: $\\langle \\x^{(1)}, \\y^{(1)} \\rangle, \\ldots, \\langle \\x^{(k)}, \\y^{(k)} \\rangle $\n",
    "- the prompt string $\\x$\n",
    "- delimiting elements by separator characters $\\langle \\text{SEP}_1 \\rangle. \\langle \\text{SEP}_2 \\rangle$\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\dot \\x = \\text{concat} (  & \\x^{(1)}, \\langle \\text{SEP}_1 \\rangle, \\y^{(1)}, \\langle \\text{SEP}_2 \\rangle,  \\\\\n",
    "              &   \\vdots \\\\\n",
    "              &   \\x^{(k)}, \\langle \\text{SEP}_1 \\rangle, \\y^{(k)}, \\langle \\text{SEP}_2 \\rangle, \\\\\n",
    "              &   \\x \\\\\n",
    "              & ) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The LLM then computes\n",
    "$$\n",
    "\\pr{ \\y | \\dot \\x }\n",
    "$$\n",
    "\n",
    "For convenience, we will just write this as the conditional probability\n",
    "$$\n",
    "\\pr{\\y | \\x,  C}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Zero shot learning: learning to learn\n",
    "\n",
    "Does Pre-Train, Prompt, Predict work ?\n",
    "\n",
    "We can begin to answer this question by\n",
    "- examining the behavior of a Pre-Trained LLM\n",
    "- on a new task\n",
    "- using $k$ exemplars\n",
    "    - varying $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Depending on $k$, we refer to the behavior of the LLM by slightly different names\n",
    "- **Few shot learning**: $10 \\le k \\le 100$ typically\n",
    "- **One shot learning**: $k = 1$\n",
    "- **Zero shot learning** $k=0$\n",
    "\n",
    "A picture will help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Few/One/Zero shot learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_Few_Shot_Training.png\"\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://arxiv.org/pdf/2005.14165.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is this even possible ?!   Learning a new task with **zero** exemplars ?\n",
    "\n",
    "Let's look at the reported In-Context Learning results of 3 LLM's of varying size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Few/One/Zero shot learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_Few_Shot_Accuracy.png\"\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://arxiv.org/pdf/2005.14165.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A couple of observations\n",
    "- Bigger models (more weights/parameters) perform better than smaller models\n",
    "    - compare the 175 Billion parameter model to the smaller models \n",
    "- More exemplars (greater $k$) helps\n",
    "    - but not much for the smallest model\n",
    "- As the size of the model grows: In-Context Learning behavior improves\n",
    "    - we sometimes refer to this as behavior that \"emerges\" only when a model is sufficiently large\n",
    "- Zero shot learning works !\n",
    "    - but this is a behavior that only emerges for very large models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Few-shot learning: let's experiment\n",
    "\n",
    "The [HuggingFace platform](https://huggingface.co/) has libraries of pre-trained models for many tasks, including Language models.\n",
    "\n",
    "There is a clean API for using these models in code (I recommend their on-line [course](https://huggingface.co/) if you want to play with it).\n",
    "\n",
    "But they also host many of their models for interactive use.\n",
    "\n",
    "This is valuable not just for the obvious reason of ease of use\n",
    "- some models are too big to load on the machines available to us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For fun, let's try using k-shot learning in order to get a Pre-Trained Language model to\n",
    "classify whether a short movie review is positive or negative.\n",
    "\n",
    "[Movie review sentiment: few shot learning GPT-2](https://huggingface.co/gpt2?text=this+movie+was+great%3A+positive%0A%0A+one+of+the+best+films+of+the+year%3A+positive+%0A%0Ajust+plain+awful%3A+negative+%0A%0AI+would+not+see+this+one+again%3A+negative+%0A%0Athis+movie+was+great%3A+positive+%0A%0Aone+of+the+best+films+of+the+year%3A+positive+%0A%0A+just+plain+awful%3A+negative+%0A%0AI+would+not+see+this+one+again%3A+negative+%0A%0AI+am+disturbed+by+this+film%3A)\n",
    "\n",
    "[Movie review sentiment: few shot learning GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6B?text=this+movie+was+great%3A+positive%0A%0A+one+of+the+best+films+of+the+year%3A+positive+%0A%0Ajust+plain+awful%3A+negative+%0A%0AI+would+not+see+this+one+again%3A+negative+%0A%0Athis+movie+was+great%3A+positive+%0A%0Aone+of+the+best+films+of+the+year%3A+positive+%0A%0A+just+plain+awful%3A+negative+%0A%0AI+would+not+see+this+one+again%3A+negative+%0A%0AI+am+disturbed+by+this+film%3A)\n",
    "\n",
    "[Movie review sentiment: few shot learning:gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b?text=this+movie+was+great%3A+positive%0A%0A+one+of+the+best+films+of+the+year%3A+positive+%0A%0Ajust+plain+awful%3A+negative+%0A%0AI+would+not+see+this+one+again%3A+negative+%0A%0Athis+movie+was+great%3A+positive+%0A%0Aone+of+the+best+films+of+the+year%3A+positive+%0A%0A+just+plain+awful%3A+negative+%0A%0AI+would+not+see+this+one+again%3A+negative+%0A%0AI+am+disturbed+by+this+film%3A)\n",
    "\n",
    "You can try cutting and pasting the prompt into the hosted inference instance of other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you click on the `Deploy` button and choose the `Inference API` drop-down\n",
    "- you will see Python code for querying the model programaticly.\n",
    "\n",
    "<img src=\"images/hf_inference_api_code.png\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Prompt engineering\n",
    "\n",
    "You see how the behavior of the LLM can be affected by the exact form of the prompt.\n",
    "- the number of exemplars\n",
    "\n",
    "It is also the case that slightly changing the words (and order of words) affects behavior.\n",
    "\n",
    "There is a whole literature on creating successful prompts: [Prompt engineering](https://arxiv.org/pdf/2107.13586.pdf), [Chain of thought prompting](https://arxiv.org/pdf/2201.11903.pdf)\n",
    "\n",
    "\n",
    "OpenAI provides [helpful examples](https://platform.openai.com/examples) for prompting.\n",
    "\n",
    "[See Appendix G](https://arxiv.org/pdf/2005.14165.pdf#page=51) (pages 50+) for examples of prompts for many other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chain of thought prompting\n",
    "\n",
    "[Paper: Chain of thought prompting](https://arxiv.org/pdf/2201.11903.pdf)\n",
    "\n",
    "In school, students are often tasked with solving problems involving multiple steps.\n",
    "\n",
    "LLM's are better at multi-step reasoning tasks when they have been conditioned to answer step by step.\n",
    "\n",
    "We call this *chain of thought (CoT)* prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The exemplars used in CoT prompting\n",
    "- demonstrate step by step reasoning in the expected output\n",
    "\n",
    "We can see the difference in the exemplar's \"Example Output\" section\n",
    "- using \"Standard Prompting\" (on the left)\n",
    "- versus using \"CoT Prompting\" (on the right)\n",
    "\n",
    "<img src=\"images/cot_prompt_example.png\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does this apply to the case of *zero* exemplars (zero-shot learning) ?\n",
    "\n",
    "It turns out that step by step reasoning can be elicited\n",
    "- Just by adding the phrase [\"Let's think step by step\"](https://arxiv.org/pdf/2205.11916.pdf) to the end of the query\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's ask ChatGPT to solve a multi-step reasoning problem in a zero-shot setting.\n",
    "\n",
    "As you can see: it comes close, by produces an incorrect answer.\n",
    "\n",
    "<img src=\"images/cot_prompt_no_step_by_step.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The reasoning logic is correct, but the math is wrong.\n",
    "\n",
    "Now, let's run the same query but append a request to answer step-by-step.\n",
    "\n",
    "<img src=\"images/cot_prompt_step_by_step.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"LETS THINK STEP BY STEP\" seems to condition the model into getting the math correct\n",
    "- Without CoT: answers comes before reasoning\n",
    "- With CoT: reasoning precedes answer\n",
    "\n",
    "It is the conditioning of providing the reasoning before the answer that seems to improve the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using zero-shot to create new applications\n",
    "\n",
    "With a little cleverness, one can almost trivially create a new application using a LLM in zero-shot mode\n",
    "- create the prefix of a prompt describing the task\n",
    "- append the user input to the prefix to complete the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we use [`ChatGPT`](https://chat.openai.com/chat) to create an app that summarizes a conversation\n",
    "- we create a prompt with a \"place-holder\" (in braces `{..}`) for user input\n",
    "\n",
    "`prompt = Summarize the following conversation: {user input}`\n",
    "\n",
    "<img src=\"images/chatgpt_summarize_conversation_example.png\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we use ChatGPT as a programming assistant\n",
    "\n",
    "`prompt = Write a Python function that does the following: {task description}`\n",
    "\n",
    "<img src=\"images/chatgpt_program_generation_example.png\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some more, creative examples\n",
    "- [Spreadsheet add-in to perform lookups](https://twitter.com/pavtalk/status/1285410751092416513)\n",
    "- [Generate a web page from a description](https://twitter.com/sharifshameem/status/1283322990625607681)\n",
    "\n",
    "References found in: http://ai.stanford.edu/blog/understanding-incontext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How is zero-shot learning possible ? Some theories\n",
    "\n",
    "**Theory 1**\n",
    "\n",
    "- The training set contains explicit instances of these out of sample tasks\n",
    "\n",
    "**Theory 2**\n",
    "\n",
    "- The super-large training sets  contain *implicit* instances of these out of sample tasks\n",
    "    - For example: an English-language article quoting a French speaker in French with English translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One thing that jumps out from the graph:\n",
    "- Bigger models are more likely to exhibit meta-learning\n",
    "\n",
    "**Theory 3**\n",
    "\n",
    "The training sets are so big that the model \"learns\" to create groups of examples with a common theme\n",
    "- Even with the large number of parameters, the model capacity does not suffice for example memorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another thing to consider\n",
    "- The behavior of an RNN depends on *all* previous inputs\n",
    "    - It has memory (latent state, etc.)\n",
    "    \n",
    "So Few Shot Learning may work by \"priming\" the memory with parameters for a specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Social concerns\n",
    "\n",
    "The team behind GPT is very concerned about potential misuse of Language Models.\n",
    "\n",
    "To illustrate, they conducted an experiment in having a Language Model construct news articles\n",
    "- Select title/subtitle of a genuine news article\n",
    "- Have the Language Model complete the article from the title/subtitle\n",
    "- Show humans the genuine and generated articles and ask them to judge whether the article was written by a human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Human accuracy in detecting model generated news articles</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_GPT_model_generated_news.png\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://arxiv.org/pdf/2005.14165.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The bars show the range of accuracy across the 80 human judges.\n",
    "\n",
    "- 86% accuracy detecting articles created by a really bad model (the control)\n",
    "- 50% accuracy detecting articles created by the biggest models\n",
    "\n",
    "It seems that humans might have difficulty distinguishing between genuine and generated articles.\n",
    "\n",
    "The fear is that Language Models can be used\n",
    "- to mislead\n",
    "- to create offensive speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
