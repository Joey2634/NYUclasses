{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Language Models\n",
    "\n",
    "A *Language Model* is an instance of the \"predict the next\" paradigm where\n",
    "- given a sequence of words\n",
    "- we try to predict the next word\n",
    "\n",
    "Recall the architecture to solve \"predict the next word\" and data preparation\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "<tr>\n",
    "    <center><strong>Language Modeling task</strong></center>\n",
    "</tr>\n",
    "    <br>\n",
    "<tr>\n",
    "    <th><center>Architecture</center></th>\n",
    "    <th><center>Data preparation</center></th>\n",
    "    </tr>\n",
    "<tr>\n",
    "    <td><img src=\"images/RNN_many_to_one_to_classifier.jpg\" width=70%></td>\n",
    "    <td><center>$\\mathbf{s} = \\mathbf{s}_{(1)}, \\ldots, \\mathbf{s}_{(T)}$</center>\n",
    "        <br><br><br>\n",
    "        \\begin{array} \\\\\n",
    "      i  & \\x^\\ip  & \\y^\\ip \\\\\n",
    "      \\hline \\\\\n",
    "      1 & \\mathbf{s}_{(1) }  & \\mathbf{s}_{(2)} \\\\\n",
    "      2 & \\mathbf{s}_{(1), (2) }  & \\mathbf{s}_{(3)} \\\\\n",
    "      \\vdots \\\\\n",
    "      i & \\mathbf{s}_{(1), \\ldots, (i) }  & \\mathbf{s}_{(i+1)} \\\\\n",
    "      \\vdots \\\\\n",
    "      (T-1) & \\mathbf{s}_{(1), \\ldots, (T-1) }  & \\mathbf{s}_{(T)} \\\\\n",
    "      \\end{array}\n",
    "    </td>\n",
    "<tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The raw data\n",
    "- e.g., the sequence of words $\\mathbf{s} = \\mathbf{s}_{(1)}, \\ldots \\mathbf{s}_{(\\bar T)}$\n",
    "\n",
    "is not naturally labeled.\n",
    "\n",
    "We need a Data Preparation step to create examples\n",
    "$$\n",
    "\\langle \\x^\\ip, \\y^\\ip \\rangle = \\langle \\mathbf{s}_{(1)}, \\ldots \\mathbf{s}_{(i)}, \\mathbf{s}_{(i+1)} \\rangle\n",
    "$$\n",
    "to create labelled examples.\n",
    "\n",
    "We have called this method of turning unlabeled data into labeled examples: *Semi-Supervised* Learning.\n",
    "\n",
    "In the NLP literature, it is called *Unsupervised Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are abundant sources of raw text data\n",
    "- news, books, blogs, Wikipedia\n",
    "- not all of the same quality\n",
    "\n",
    "The large number of examples that can be generated facilitates the training of models with very large number of weights.\n",
    "\n",
    "This is extremely expensive but, fortunately, the results can be re-used.\n",
    "- Someone with abundant resources trains a Language Model on a broad domain\n",
    "- Publishes the architecture and weights\n",
    "- Others re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Models that have been trained with the intent that they be re-used are called *Pre-Trained* models.\n",
    "\n",
    "The process of creating such a Language Models\n",
    "from unlabeled raw text is referred to as \n",
    "\n",
    "> *Unsupervised Pre-Training*: Train a model on a **very large** number of examples from a **broad** domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using a Pre-Trained Language Model\n",
    "\n",
    "## Feature based\n",
    "\n",
    "Consider the behavior of a Language Model as it processes a word sequence (either an RNN or Encoder Transformer).\n",
    "\n",
    "It produces an output (or latent state) $\\bar\\h_\\tp$ for each position $\\tt$ of the sequence.\n",
    "\n",
    "This is a *context sensitive* representation specific to input word $\\mathbf{s}_tp$ at position $\\tt$.\n",
    "- context sensitive because it depends on\n",
    "    - prefix $\\mathbf{s}_{(1)}, \\ldots, \\mathbf{s}_{(\\tt-1)}$\n",
    "    - entire sequence $\\mathbf{s}_{(1)}, \\ldots, \\mathbf{s}_{(\\bar T)}$\n",
    "\n",
    "These Context Sensitive Representations of words may be useful representations for down-stream tasks\n",
    "- Better than Word Embeddings, which have no context\n",
    "- See the [ELMo paper](https://arxiv.org/abs/1802.05365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "\n",
    "\n",
    "Logically, we use the process that we described as Transfer Learning\n",
    "- where we use the output of some layer of the Pre-Trained model\n",
    "    - default: all layers, excluding the Classification Head\n",
    "- as a \"meaningful\" **fixed length** representation of input sequence $\\x^\\ip_{(1)}, \\ldots, \\x^\\ip_{(m)}$\n",
    "- which is then fed to a Classification head with the object of matching the target $\\y^\\ip$\n",
    "\n",
    "Recall the diagram from our module on [Transfer Learning](Transfer_Learning.ipynb)\n",
    "\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: replace the head of the pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transfer_Learning_2.jpg\" width=60%></td>\n",
    "    </tr>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The process is\n",
    "- Import the Pre-Trained model (which was trained on a large number of examples from a broad domain)\n",
    "- Fine-Tune  the weights using a **small** number of examples for a **specific task** from a **narrow** domain.\n",
    "\n",
    "Often, the specific task is Supervised (e.g., sentiment analysis).\n",
    "\n",
    "In that case: we refer to the second step as *Supervised Fine-Tuning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Using a Pre-trained Language Model to analyze sentiment\n",
    "\n",
    "This is a straight-forward application of Transfer Learning\n",
    "- Replace the Classification Head used for Language Modeling\n",
    "    - e.g., a head that generated a probability distribution over words in the vocabulary\n",
    "- By an un-trained Binary Classification head (Positive/Negative sentiment)\n",
    "- Train on examples. Pairs of\n",
    "    - sentence\n",
    "    - label: Positive/Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Language Models: the future (present ?) of NLP ?\n",
    "\n",
    "Pre-trained Language Models (especially the *Large Language Models* that have trained on massive amounts of data) seem to transfer well to other tasks via Supervised Fine-Tuning.\n",
    "\n",
    "We call this paradigm \"Unsupervised Pre-Trained Model + Supervised Fine-Tuning\".\n",
    "\n",
    "This paradigm means that we might not need to create a new model for a new task.\n",
    "\n",
    "Instead: we transform our task into one amenable to the \"Unsupervised Pre-Trained Model + Supervised Fine-Tuning\" paradigm\n",
    "- using a Language Model as our Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Input Transformations\n",
    "\n",
    "One impediment to using the  paradigm is that\n",
    "- the task-specific input\n",
    "- is  not the simple, unstructured sequence of words that characterize the input for Language Modeling.\n",
    "\n",
    "We need to apply *input transformations*\n",
    "- to transform structured task-specific input\n",
    "- to the unstructured sequence of words used in the Language Model task input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are common examples of tasks with structured inputs:\n",
    "- Entailment\n",
    "    - Input is a *pair* of text sequences $[ \\text{Premise}, \\text{Hypothesis} ]$\n",
    "    - Binary classification: Does the Hypothesis Logically follow from the Premise ?\n",
    "    \n",
    "          Premise: The students are attending a lecture on Machine Learning\n",
    "          Hypothesis: The students are sitting in a class room\n",
    "          Label: Entails\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Question answering\n",
    "    - Consider a multiple choice questions consisting of\n",
    "        - Context: a sentence or paragraph stating facts\n",
    "        - Question\n",
    "        - Answers: a set of possible answer sentences\n",
    "    - Input\n",
    "    \n",
    "    \n",
    "    Context: It is December of 2022.  Prof. Perry is teaching the second half of the Machine Learning Course.\n",
    "    Question: Where are the students ?\n",
    "    Answer 1: The beach\n",
    "    Answer 2: In a classroom in Brooklyn\n",
    "    Answer 3: Dreaming of being elsewhere.\n",
    "    \n",
    "    Label: Answer 2 (95% probability), Answer 3 (4% probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Similarity  \n",
    "    - Input is a *pair* (or more) of text sequences $[ \\text{First}, \\text{Second}, \\text{Third} ]$\n",
    "    - Binary/Multinomial  classification: Probability that other sentences are similar to First ? \n",
    "    \n",
    "    \n",
    "        First: Machine Learning is easy not hard\n",
    "        Second: Machine Learning is not difficult\n",
    "        Third:  Machine Learning is hard not easy\n",
    "        Label: [Second: .95, Third: .01 ]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To use the Pre-Trained LM + Fine-Tuning approach\n",
    "- we need to convert the structured input into simple sequences.\n",
    "\n",
    "See [this paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) for some transformations.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>GPT: Task encoding</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_GPT_task_encoding.png\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example: for multiple choice questions answering\n",
    "- Create a triple for each answer, \n",
    "    - [Context, Question, Answer 1], \n",
    "    - [Context, Question, Answer 2], ...\n",
    "- Obtain a representation of each triple using a LM\n",
    "    - using Delimiter tokens to separate elements of the triple\n",
    "- Fine-tune using a new Multinomial classifier head\n",
    "    - to obtain probability distribution over answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Language Models are the basis for the paradigm of Unsupervised Pre-training + Supervised Fine-Tuning.\n",
    "\n",
    "This has become the dominant paradigm in NLP.\n",
    "\n",
    "The ability to train Large Language models stem is due, in part, to the advantages of the Transformer\n",
    "- Execution Parallelism: can run larger models than an RNN for the same amount of elapsed time\n",
    "- This also facilitates the use of extremely large training datasets.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
