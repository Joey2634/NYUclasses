<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Keith A. Lewis">
  <title>Random Variable</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="math.css">
  <script src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
    var mathElements = document.getElementsByClassName("math");
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") { katex.render(texText.data, mathElements[i], { displayMode: mathElements[i].classList.contains("display"), throwOnError: false } );
    }}});</script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css">
</head>
<body>
<html><head></head><body><header id="title-block-header">
<h1 class="title">Random Variable</h1>
<p class="author">Keith A. Lewis</p>
</header>
<p>A <em>random variable</em> is a variable, a symbol that takes the place of number in an equation or inequality, together with the probability of the values it can have. Random variables are specified by their <em>cumulative distribution function</em> <span class="math inline">F^X(x)</span>, or just <span class="math inline">F(x)</span>, if <span class="math inline">X</span> is understood, giving the probability <span class="math inline">X\le x</span>.</p>
<p>Every cdf must satisfy <span class="math inline">0\le F\le 1</span> since it is a probability and is nondecreasing, <span class="math inline">F(x) \le F(y)</span> if <span class="math inline">x < y</span>, since <span class="math inline">X\le x</span> implies <span class="math inline">X\le y</span>.</p>
<p><strong>Exercise</strong> <em>Show every cdf is right continuous</em>.</p>
<p><em>Hint</em>. This means <span class="math inline">\lim_{h\downarrow 0} F(x + h) = F(x)</span>. Use <span class="math inline">(-\infty, x] = \cap_{h>0} (-\infty, x + h]</span>.</p>
<p><strong>Exercise</strong> <em>Show every cdf has left limits</em>.</p>
<p><em>Hint</em>. This means <span class="math inline">\lim_{h\downarrow 0} F(x - h)</span> exists. Use <span class="math inline">(-\infty, x) = \cup_{h>0} (-\infty, x - h]</span> and every bounded set in <span class="math inline">{\boldsymbol{R}}</span> has a <em>least upper bound</em>.</p>
<p>Any function satsifying these properties is the cdf of a random variable.</p>
<h2 id="expected-value">Expected Value</h2>
<p>The <em>expected value</em>, or <em>mean</em>, of <span class="math inline">X</span> is the Riemann-Stieljes integral <span class="math inline">E[X] = \int_{\boldsymbol{R}}x\,dF(x)</span>. The greek letter <span class="math inline">μ</span> is typically used for this. Expected value is a <em>measure of central tendency</em> providing a single number indicating the <em>location</em> of <span class="math inline">X</span>. The <em>median</em> of <span class="math inline">X</span> is the number <span class="math inline">m</span> with <span class="math inline">F(m) = P(X \le m) = 1/2</span> provides a more <em>robust</em> measure of location. Values of <span class="math inline">F(x)</span> for <span class="math inline">x > m</span> or <span class="math inline">x < m</span> do not change the median.</p>
<p>The <em>expected value</em> of <span class="math inline">f(X)</span> for a function <span class="math inline">f\colon{\boldsymbol{R}}\to{\boldsymbol{R}}</span> is <span class="math inline">E[f(X)] = \int_{\boldsymbol{R}}f(x)\,dF(x)</span>. Since <span class="math inline">F</span> has <em>bounded variation</em> this exists whenever <span class="math inline">f</span> is continous.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">f</span> is continuous show <span class="math inline">\int_{{\boldsymbol{R}}} f\,dF = f(a)</span> when <span class="math inline">F(x) = 1(x < a)</span></em>.</p>
<p><em>Hint</em>. All terms in the Riemann-Stieljes integral are zero except for the interval containing <span class="math inline">a</span>.</p>
<p>If <span class="math inline">A</span> is a set the <em>indicator function</em> <span class="math inline">1_A(x)</span> is <span class="math inline">1</span> when <span class="math inline">x\in A</span> and <span class="math inline">0</span> when <span class="math inline">x\not\in A</span>. If <span class="math inline">P(x)</span> is a <em>proposition</em> involving <span class="math inline">x</span> then <span class="math inline">1(P(x))</span> is <span class="math inline">1</span> when <span class="math inline">P(x)</span> is true and <span class="math inline">0</span> when <span class="math inline">P(x)</span> is false.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">X = a</span> with probablity one show <span class="math inline">F(x) = 1(x < a)</span></em>.</p>
<p>This is also written as <span class="math inline">dF(x) = δ_a(x)</span>, the <em>delta function</em> with unit mass at <span class="math inline">a</span>. Delta functions are not functions, they are <em>set functions</em>. Given a set <span class="math inline">A</span> the delta “function” <span class="math inline">δ_a</span> yields the value <span class="math inline">1</span> if <span class="math inline">a\in A</span> and <span class="math inline">0</span> if <span class="math inline">a\not\in A</span>.</p>
<p>A random variable <span class="math inline">X</span> is <em>discrete</em> if <span class="math inline">P(X = x_j) = p_j</span> where <span class="math inline">p_j > 0</span> and <span class="math inline">\sum_j p_j = 1</span>. The data <span class="math inline">(x_j, p_j)</span> completely specify the random variable.</p>
<p>A discrete random variable that can only take on values 0 or 1 is called <em>Bernoulli</em>. A Bernoulli random variable is specified by a single number <span class="math inline">p\in(0, 1)</span> where <span class="math inline">P(X = 1) = p</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">P(X = 0) = 1 - p</span></em>.</p>
<p>A random variable <span class="math inline">X</span> is <em>continuously distributed</em> if <span class="math inline">F(x) = \int_{-\infty}^x dF(u) = \int_{-\infty}^x F'(u)\,du</span>. The function <span class="math inline">f(x) = F'(x)</span> is the <em>density function</em> of the random variable. The <em>mode</em> of a continuously distributed random variable is where the derivative of <span class="math inline">f</span> is zero. It is another measure of location, but there may no, or many, values at which this occurs.</p>
<h2 id="moments">Moments</h2>
<p>The <span class="math inline">n</span>-th <em>moment</em> of <span class="math inline">X</span> is <span class="math inline">μ_n = E[X^n]</span> when it exists. The <em>moment generating function</em> of <span class="math inline">X</span> is <span class="math inline">m(s) = E[e^{sX}] = \sum_{n=0}^\infty μ_n s^n/n!</span>. It is possible for two unequal random variables to have the same moments <span class="citation" data-cites="cite">[@cite]</span>.</p>
<p>The <span class="math inline">n</span>-th <em>central moment</em> of <span class="math inline">X</span> is <span class="math inline">\bar{μ}_n = E[(X - E[X])^n]</span>. The second central moment is the <em>variance</em>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">\operatorname{Var}(X) = E[(X - E[X])^2] = E[X^2] - E[X]^2</span></em>.</p>
<p><em>Hint</em>. Expectation is an integral so it is linear, <span class="math inline">E[aX + b] = aE[X] + b</span>.</p>
<p>Variance is a measure of <em>dispersion</em>, how far the random variable can stray from its mean. The <em>standard deviation</em> is the square root of the variance and has the same units as <span class="math inline">X</span>. The greek letter <span class="math inline">σ</span> is typically used for the standard deviation.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">\operatorname{Var}(X) = 0</span> show <span class="math inline">P(X = E[X]) = 1</span></em>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">X</span> has first and second moments then <span class="math inline">(X - μ)/σ</span> has mean zero and variance one</em>.</p>
<p>Subtracting the mean and dividing by the standard deviation is called <em>standardizing</em>.</p>
<p>The third moment of a standardized random variable is called <em>skewness</em>, a measure of how lopsided a distribution is.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">X</span> and <span class="math inline">-X</span> have the same distribution then its skewness is zero</em>.</p>
<p>The fourth moment of a standardized random variable is called <em>kurtosis</em>, a measure of how peaked a distribution is.</p>
<p><strong>Exercise</strong>. <em>Show the <span class="math inline">2n</span>-th moment of a standard normal is $(2n)!/2^n n!</em>.</p>
<p><em>Hint</em>. <span class="math inline">E[e^{sX}] = e^{s^2/2}</span>.</p>
<details>
<summary>Solution</summary>
We have <span class="math inline">\sum_0^\infty μ_n s^n/n! = μ(s) = e^{s^2/2} = \sum_0^\infty (s^2/2)^n/n!</span>. Equating coefficients, <span class="math inline">μ_{2n}/(2n)! = (1/2)^n/n!</span> so <span class="math inline">μ_{2n} = (2n)!/2^n n!</span>.</details>
<h2 id="cumulants">Cumulants</h2>
<p>The logarithm of the moment generating function is the <em>cumulant</em> <span class="math inline">κ(s) = \log E[e^{sX}] \sum_{n=1}^\infty κ_n s^n/n!</span>. The coefficients <span class="math inline">(κ_n)</span> are the <em>cumulants</em>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">κ(0) = 0</span>, <span class="math inline">κ'(0) = E[X]</span>, and <span class="math inline">κ''(0) = \operatorname{Var}(X)</span></em>.</p>
<p>Moments and cumulants are related by Bell polynomials.</p>
<p><span class="math inline">\exp(\sum_{n=1}^\infty \kappa_n s^n/n!) = \sum_{n=0}^\infty m_n s^n/n!</span>.</p>
<h2 id="inequalities">Inequalities</h2>
<p>The mean of a random variable places a contraint on its upper tail.</p>
<p><strong>Lemma</strong>. (Markov) <em>For any random variable <span class="math inline">X</span>, <span class="math inline">P(X > λ) \le E[X]/λ</span></em>.</p>
<p><em>Proof</em>. <span class="math inline">E[X] \ge E[X1(X\ge λ)] \ge λ P(X\ge λ)</span>.</p>
<p><strong>Exercise</strong>. (Chebychev) <em>Show <span class="math inline">P(|X - E[X]| > λ) \le \operatorname{Var}(X)/λ^2</span></em>.</p>
<p><em>Hint</em>. Replace <span class="math inline">X</span> by <span class="math inline">(X - E[X])^2</span> and <span class="math inline">λ</span> by <span class="math inline">λ^2</span>.</p>
<h3 id="concentration-inequalities">Concentration Inequalities</h3>
<p>Let <span class="math inline">(X_j)</span> be independent, identically distributed random variables with mean <span class="math inline">μ</span> and standard deviation <span class="math inline">σ</span>. and define the average <span class="math inline">S_n = (1/n)\sum_{j=1}^n X_j</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">E[S_n] = μ</span> and <span class="math inline">\operatorname{Var}(S_n) = σ^2/n</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show for any <span class="math inline">ε > 0</span> that <span class="math inline">\lim_{n\to\infty} P(|S_n - μ| > ε) = 0</span></em>.</p>
<p>This is the <em>weak law of large numbers</em>; the average tends to the mean <em>in probability</em>.</p></body></html>
</body></html>