{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1\n",
    "**Plan**\n",
    "- Motivate Machine Learning\n",
    "- Introduce notation used throughout course\n",
    "- Plan for initial lectures\n",
    "    - *What*: Introduce, motivate a model\n",
    "    - *How*:  How to use a model: function signature, code (API)\n",
    "    - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
    "\n",
    "        \n",
    "- [Course Overview](Course_overview_NYU.ipynb)\n",
    "\n",
    "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
    "- [Intro to Classical ML](Intro_Classical_ML.ipynb)\n",
    "\n",
    "**Week 2 (early start)**\n",
    "\n",
    "**Plan**\n",
    "- Introduce a model for the Regression task: Linear Regression\n",
    "- Introduce the Recipe for Machine Learning: detailed steps to problem solving\n",
    "\n",
    "\n",
    "- [Our first model: Linear Regression (Overview)](Linear_Regression_Overview.ipynb)\n",
    "- A *process* for Machine Learning\n",
    "    - Go through the methodical, multi-step process\n",
    "        - Quick first pass, followed by Deeper Dives\n",
    "    - This will be a code-heavy notebook !\n",
    "    - Illustrate Pandas, Jupyter, etc\n",
    "    \n",
    "**The Recipe for Machine Learning, illustrated with the Linear Regression model**  \n",
    "- [Recipe for Machine Learning: Overview](Recipe_Overview.ipynb)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2\n",
    "**Plan**\n",
    "\n",
    "We will continue learning the Recipe for Machine Learning, illustrated by introducing the Linear Regression model for solving Regression tasks.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L3_S4_ML_Process.png\" width=\"100%\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Recipe for Machine Learning/Linear Regression** (continued)\n",
    "\n",
    "- We continue with Step 2 of the Recipe: [Exploratory Data Analysis](Recipe_Overview.ipynb#Exploratory-Data-Analysis)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)\n",
    "    \n",
    "    \n",
    "- [Linear Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
    "\n",
    "**Transformations**\n",
    " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    " \n",
    "**Deeper dives**\n",
    "- Iterative improvement\n",
    "    - [When to stop: Bias and Variance](Bias_and_Variance.ipynb)\n",
    "        - Regularization\n",
    "- [Fine tuning techniques](Fine_tuning.ipynb)\n",
    "\n",
    "**Week 3 (early start)**\n",
    "\n",
    "**Plan**\n",
    "- Introduce a model for the Classification task: Logistic Regression\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview](Classification_Overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3\n",
    "\n",
    "**Regression wrap-up**\n",
    "\n",
    "A few missing items/clarifications from last week\n",
    "- Use linear regression for hedging (clarification).  I gave the linear regression example\n",
    "$$\n",
    "r_\\text{AAPL} \\approx \\Theta_0 + \\Theta_1 * r_\\text{SPX}\n",
    "$$\n",
    "    - The return of AAPL is $\\Theta_1$ times as much as the return of the index SPX\n",
    "    - So if I hold \\\\$1 of AAPL long, I would short $\\Theta_1$ of SPX in order to neutralize market exposure\n",
    "    - Often assume $\\Theta_0 = 0$\n",
    "    - Remember: the fit involves an \"error\"\n",
    "    $$\n",
    "r_\\text{AAPL} = \\Theta_0 + \\Theta_1 * r_\\text{SPX} + \\epsilon\n",
    "$$\n",
    "    - The hedged portfolio returns $\\epsilon$\n",
    "    - If, through my modeling, I conclude that $\\epsilon > 0$, I can profit regardless of the direction of SPX\n",
    "\n",
    "- Where do regression errors come from ?\n",
    "    - Measurement error, noise\n",
    "    - Missing feature\n",
    "    \n",
    "- [Regularization: a way to combat overfitting](Bias_and_Variance.ipynb#Regularization:-reducing-overfitting)\n",
    "\n",
    "**Plan**\n",
    "- We continue our Introduction of a model for the Classification task: Logistic Regression\n",
    "- How to deal with Categorical (non-numeric) variables\n",
    "    - classification target\n",
    "    - features\n",
    "\n",
    "<img src='images/class_overview_prob_lines.jpg'>\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview (continued)](Classification_Overview.ipynb#Logistic-Regression)\n",
    "- [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "**Classification, continued**\n",
    "- [Multinomial Classification](Multinomial_Classification.ipynb)\n",
    "- [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [Log odds](Classification_Log_Odds.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4\n",
    "\n",
    "**Plan**\n",
    "- Classsification and Categorical variables wrapup\n",
    "    - Baseline models: a baseline model for classification\n",
    "    - OHE and Linear Regression: The Dummy Variable Trap\n",
    "- Error Analysis\n",
    "\n",
    "**Classsification and Categorical variables wrapup**\n",
    "- [Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "- [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "\n",
    "\n",
    "**Wrap-up from last week: Multinomial classification, illustrated**\n",
    "\n",
    "Last week, we indicated how we could use a collection of binary classifiers to create a classifier for more than $2$ classes.\n",
    "\n",
    "[Here](Multinomial_Classification.ipynb#Multinomial-classification-example:-MNIST-digit-classifier)\n",
    "is an example to illustrate the keys concepts, as well as better understand the $\\Theta$ as patterns being matched in the input examples\n",
    "\n",
    "**This week**\n",
    "\n",
    "Good news\n",
    "- You now know two main tasks in Supervised Learning\n",
    "    - Regression, Classification\n",
    "- You now know how to use virtually every model in `sklearn`\n",
    "    - Consistent API\n",
    "        - `fit`, `transform`, `predict`\n",
    "- You survived the \"sprint\" to get you up and running with ML\n",
    "- You know the *mechanical process* to implement transformations: Pipelines\n",
    "    \n",
    "**Plan**\n",
    "- Error Analysis\n",
    "    - We explain Error Analysis for the Classification Task, with a detailed example\n",
    "    - How Training Loss can be improved\n",
    "- Transformations, continued\n",
    "    - One of the most important parts of the Recipe: transforming raw data into something that tells a story\n",
    "- Loss functions\n",
    "    - We look at the mathematical logic behind loss functions\n",
    "\n",
    "**Error Analysis**\n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [linked notebook](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "    - [Worked example](Error_Analysis_MNIST.ipynb)\n",
    "\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    " \n",
    "**Transformations, continued**\n",
    "- [Transformations Overview](Transformations_Overview.ipynb)\n",
    "\n",
    "**Imbalanced data**\n",
    "- [Imbalanced data](Imbalanced_Data.ipynb)\n",
    "\n",
    "\n",
    "**Loss functions: mathematical basis**\n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "- [Loss functions: the math](Loss_functions.ipynb)\n",
    "    - Maximum likelihood\n",
    "    - Preview: custom loss functions and Deep Learning\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5\n",
    "\n",
    "**Plan**\n",
    "- More models: Decision Trees, Naive Bayes\n",
    "    - Different flavor: more procedural, less mathematical\n",
    "    - Decision Trees: a model with *non-linear* boundaries\n",
    "- Ensembles\n",
    "    - Bagging and Boosting\n",
    "    - Random Forests\n",
    "- Naive Bayes: a simple but effective model\n",
    "\n",
    "**Decision Trees, Ensembles**\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)\n",
    "\n",
    "**Naive Bayes**\n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n",
    "**Week 6 (early start)**\n",
    "\n",
    "**Unsupervised Learning: PCA**\n",
    "- [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Unsupervised.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6   \n",
    "\n",
    "**Plan**\n",
    "- We continue with Unsupervised Learning: Principal Components (started last week)\n",
    "- Support Vector Classifiers: a classifier with an interesting twist \n",
    "- Gradient Descent: how to minimize a Loss function\n",
    "\n",
    "**Unsupervised Learning: PCA**\n",
    "- **Done** [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb#PCA:-The-math)(continued)\n",
    "    - [linked notebook](Unsupervised.ipynb)\n",
    "- [PCA in Finance](PCA_Yield_Curve_Intro.ipynb)\n",
    "  \n",
    "**Support Vector Classifiers**\n",
    "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)  \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n",
    "**Gradient Descent** \n",
    "- [Gradient Descent](Gradient_Descent.ipynb)\n",
    "\n",
    "**Deeper Dives**\n",
    "- [Missing data: clever ways to impute values](Missing_Data.ipynb)\n",
    "- [Other matrix factorization methods](Unsupervised_Other_Factorizations.ipynb)\n",
    "- [Feature importance](Feature_Importance.ipynb)\n",
    "- [SVC Loss function derivation](SVM_Derivation.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7\n",
    "\n",
    "**Plan**   \n",
    "- Finish Gradient Descent\n",
    "- Interpretation: understanding models\n",
    "\n",
    "\n",
    "**Gradient Descent (continued)**\n",
    "- [Gradient Descent: continued](Gradient_Descent.ipynb#Gradient-Descent:-Overview)\n",
    "- [Tensors: Matrix gradients](Matrix_Gradient.ipynb)\n",
    "\n",
    "**Interpretation**\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "  \n",
    "**Deeper dives**\n",
    "\n",
    "## Deep Learning: Headstart\n",
    "\n",
    "### Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Deep Learning/Neural networks\n",
    "\n",
    "- [Set up your Tensorflow environment](Tensorflow_setup.ipynb)\n",
    "- [Neural Networks Overview](Neural_Networks_Overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Deep Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "# Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Continue our introduction to Neural Networks/Deep Learning\n",
    "\n",
    "- Neural network theory\n",
    "- Neural network: practical\n",
    "\n",
    "**Interpretation** (forgot to do it last week)\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "\n",
    "**Neural network theory**\n",
    "- [A neural network is a Universal Function Approximator](Universal_Function_Approximator.ipynb)\n",
    "\n",
    "\n",
    "**Training Neural Networks (introduction)**\n",
    "- [Training Neural Networks - Back propagation](Training_Neural_Network_Backprop.ipynb)\n",
    "\n",
    "**Neural network: practical**\n",
    "- Coding Neural Networks: Tensorflow, Keras\n",
    "    - [Intro to Keras](Tensorflow_Keras.ipynb)\n",
    "\n",
    "- Practical Colab\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (neural_net_helper.py) --->\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (Colab_practical.ipynb)) --->\n",
    "   - **Colab**: [Practical Colab Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2022/blob/master/Colab_practical.ipynb)\n",
    " \n",
    "   \n",
    "**Deeper Dives**\n",
    "<!--- #include (Raw_TensorFlow.ipynb)) --->\n",
    "- [Keras, from past to present](Tensorflow_Keras_Archaeology.ipynb)\n",
    "- [History/Computation Graphs: Tensorflow version 1](DNN_TensorFlow_Using_TF_version_1.ipynb)\n",
    "- [Raw_TensorFlow example Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2022/blob/master/Raw_TensorFlow.ipynb) (**Colab**)\n",
    "- [Computation Graphs](Computation_Graphs.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2 Convolutional Neural Networks\n",
    "\n",
    "\n",
    "**Review of Midterm Project**\n",
    "\n",
    "**Plan**\n",
    "- The Convolutional Neural Network layer type\n",
    "- We start to answer the question posed in our introduction to Neural Networks:\n",
    ">What took so long ?\n",
    "\n",
    "\n",
    "## Convolutional Neural Networks \n",
    "\n",
    "\n",
    "**Convolutional Neural Networks (CNN)**\n",
    "- [Introduction to CNN](Intro_to_CNN.ipynb)\n",
    "\n",
    "**Convolutional Neural Networks (CNN)**\n",
    "- [CNN: multiple input/output features](CNN_Overview.ipynb)\n",
    "    - [CNN pictorial](CNN_pictorial.ipynb)\n",
    "- [CNN: Space and Time](CNN_Space_and_Time.ipynb)\n",
    "    - [CNN example from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2022/blob/master/CNN_demo.ipynb) (**Colab**) \n",
    "    - [CNN example from github](CNN_demo.ipynb) (**local machine**) \n",
    "\n",
    "**Deeper dives**\n",
    "- [Convolution as Matrix Multiplication](CNN_Convolution_as_Matrix_Multiplication.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3 Recurrent Neural Networks\n",
    "\n",
    "**Plan**\n",
    "- Why training a Neural Network can be difficult: fine-details of training\n",
    "- Introduce a new layer type: Recurrent layers\n",
    "    - Part of our \"sprint\": final layer type\n",
    "    - Will revisit more theoretical issues in subsequent lectures\n",
    "\n",
    "\n",
    "**Training Neural Networks: the fine details**\n",
    "- [Why TensorFlow ?: Gradients made easy](Training_Neural_Network_Operation_Forward_and_Backward_Pass.ipynb)\n",
    "- [Training Neural Networks: fine details](Training_Neural_Networks_Overview.ipynb)\n",
    "\n",
    "\n",
    "**Recurrent Neural Networks (RNN)**\n",
    "- [Introduction to Recurrent Neural Network (RNN)](Intro_to_RNN.ipynb)\n",
    "- [Recurrent Neural Network Overview](RNN_Overview.ipynb)\n",
    "- [Using an RNN for Generative AI](RNN_generative.ipynb)\n",
    "    - [LSTM_text_generation from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2022/blob/master/Keras_examples_LSTM_text_generation.ipynb) (**Colab**)\n",
    "    - [LSTM_text_generation from github](Keras_examples_LSTM_text_generation.ipynb) (**local machine**)\n",
    "\n",
    "**Deferred to following week**\n",
    "- [A state of the art Generative model](RNN_in_action.ipynb#Generative-text:-state-of-the-art)\n",
    "\n",
    "**RNN: Issues**\n",
    "- [Gradients of an RNN](RNN_Gradients.ipynb)\n",
    "\n",
    "\n",
    "Sprint is over ! We have covered the basic layer types; time for you to learn by experimenting.\n",
    "\n",
    "**Review of layer types**\n",
    "- [What layer type to choose](Neural_Network_Layer_Review.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [RNN: How to deal with long sequences](RNN_Long_Sequences.ipynb)\n",
    "- [Tensors: Matrix gradients](Matrix_Gradient.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4 Advanced Recurrent Architectures\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Finish up the \"vanilla\" RNN.\n",
    "- Start off with a **much simpler** code example\n",
    "- Re-introduce the more complex example of last week (Generative AI)\n",
    "\n",
    "The \"vanilla\" Recurrent Neural Network (RNN) layer we learned is very much exposed to the problem of vanishing/exploding gradients.\n",
    "\n",
    "We will review the issue and demonstrate a related layer type (the LSTM) designed to mitigate the problem.\n",
    "\n",
    "**RNN (continued)**\n",
    "- [Intro and examples: second try](Intro_to_RNN.ipynb#Recurrent-Neural-Network-(RNN)-layer)\n",
    "- [Simple code for an RNN](RNN_Overview.ipynb#RNN-in-action)\n",
    "- [RNN as a Generative model](RNN_generative.ipynb)\n",
    "    - [LSTM_text_generation from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2022/blob/master/Keras_examples_LSTM_text_generation.ipynb) (**Colab**)\n",
    "    - [LSTM_text_generation from github](Keras_examples_LSTM_text_generation.ipynb) (**local machine**)\n",
    "\n",
    "- [Gradients of an RNN](RNN_Gradients.ipynb)\n",
    "- [RNN: Gradients that Vanish/Explode](RNN_Vanishing_and_exploding_gradients.ipynb)\n",
    "- [Residual connections](RNN_Residual_Networks.ipynb)\n",
    "\n",
    "\n",
    "Sprint is over ! We have covered the basic layer types; time for you to learn by experimenting.\n",
    "\n",
    "**Review of layer types**\n",
    "- [What layer type to choose](Neural_Network_Layer_Review.ipynb)\n",
    "\n",
    "\n",
    "**LSTM: An improved RNN**\n",
    "- [Neural Programming](Neural_Programming.ipynb)\n",
    "- [Introduction to the LSTM](Intro_to_LSTM.ipynb)\n",
    "- [LSTM Overview](LSTM_Overview.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5 Transformers; Transfer Learning\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We present Attention, a way to enhance the power of RNN's, which is heavily used in a new layer type for sequence processing: the Transformer.  \n",
    "\n",
    "The Transformer layer type is now predominant in the area of Natural Language Processing (NLP).\n",
    "We give a quick introduction but we will revisit it in the module on advanced NLP.\n",
    "\n",
    "We  present an extremely useful trick (Transfer Learning) for leveraging the hard work that others have done.\n",
    "\n",
    "\n",
    "**Attention**\n",
    "- [Attention](Intro_to_Attention.ipynb)\n",
    "- [Implementing Attention](Attention_Lookup.ipynb)\n",
    "   \n",
    "**Transformers**\n",
    "- [Transformer](Transformer.ipynb)\n",
    "\n",
    "**Transfer Learning**\n",
    "- [Transfer Learning](Transfer_Learning.ipynb)\n",
    "\n",
    "     - [Transfer Learning example from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2022/blob/master/TransferLearning_demo.ipynb) (**Colab**)\n",
    "     - [Transfer Learning example from github](TransferLearning_demo.ipynb) (**local machine**)\n",
    "\n",
    "     - [Utility notebook](Dogs_and_Cats_reformat.ipynb)\n",
    "         - Takes the *very large* raw data (from Kaggle) used in the Transfer Learning example\n",
    "         - Creates a much smaller subset, using a different directory structure\n",
    "         - The above notebook uses this reorganized, smaller subset\n",
    "\n",
    "\n",
    "**Deeper dives**\n",
    "- Transformer\n",
    "    - [Keras example: pre-defined Attention layers](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
    "        - [notebook](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_with_transformer.ipynb)\n",
    "    - [TensorFlow tutorial: implements Attention, positional encoding](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "       - [notebook](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "**Further reading**\n",
    "- Attention\n",
    "    - [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "    - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- Transfer Learning    \n",
    "    - [Sebastian Ruder: Transfer Learning](https://ruder.io/transfer-learning/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6 NLP; Generative ML; Advanced Keras\n",
    "\n",
    "**Plan**\n",
    "\n",
    "(Transformer summary)\n",
    "\n",
    "We summarize the main advantages of the Transformer, which have come to dominate NLP.\n",
    "\n",
    "(NLP)\n",
    "\n",
    "We will make an initial pass on the topic of learning from text: Natural Language Processing.\n",
    "\n",
    "The first pass will use well-established techniques that are relatively easy to follow.\n",
    "\n",
    "We then explore some recent advances that have greatly increased the power of NLP.\n",
    "\n",
    "The Transformer architecture is a key contributor.\n",
    "\n",
    "\n",
    "**Transformers**\n",
    "- [summary](Transformer.ipynb#Conclusion)\n",
    "\n",
    "**Learning from text: Deep Learning for Natural Language Processing (NLP)**\n",
    "- [Natural Language Processing Overview](NLP_Overview.ipynb)\n",
    "    - [NLP from github (Colab)](https://colab.research.google.com/github/kenperry-public/ML_Fall_2022/blob/master/Keras_examples_imdb_cnn.ipynb)\n",
    "    - [NLP from github (local machine)](Keras_examples_imdb_cnn.ipynb)\n",
    "   \n",
    "\n",
    "**Evolution of Word representations**\n",
    "- [How to represent a word: syntax](NLP_Tokenization.ipynb)\n",
    "- [How to represent a word: meaning](NLP_Word_Representations.ipynb)\n",
    "\n",
    "**Language Models: the future (present ?) of NLP ?**\n",
    "- [Language Models](NLP_Language_Models.ipynb)\n",
    "- [Large Language Models](NLP_Large_Language_Models.ipynb)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7 Advanced Topics\n",
    "\n",
    "**Plan**  \n",
    "\n",
    "(NLP)\n",
    "\n",
    "We conclude the module on Large Language Models with the question:\n",
    "> Can a model trained for one task *also* solve other tasks, including ones it has not seen in training ?\n",
    "\n",
    "(Generative ML)\n",
    "\n",
    "We revisit the topic of Generative (as opposed to Discriminative) ML through our study of Autoencoders\n",
    "and Generative Adversarial Networks (GANs).\n",
    "\n",
    "- Generative ML not only allows us to create fun things (like stories and images) but also\n",
    "is a way of creating *synthetic data* which which to augment small training data sets.\n",
    "\n",
    "- (We will defer examining the code references to the module on Advanced Keras)\n",
    "\n",
    "(Advanced Keras)\n",
    "\n",
    "It turns out that we can't implement these models using the Sequential model of Keras that we have been\n",
    "using all semester:\n",
    "- it's time to give a peek at \n",
    "the Functional model.\n",
    "\n",
    "(Interpretation)\n",
    "\n",
    "As we've hinted at before: the art of Deep Learning is writing a Loss function that captures the semantics of the problem you want to solve.  \n",
    "\n",
    "- We look at more advanced Keras techniques that allow us to write complex Loss functions.\n",
    "- Additionally, we will look at other advanced features such as\n",
    "    - Custom training loop\n",
    "    - Cool things you can do with Gradients\n",
    "    - Defining your own layer type\n",
    "\n",
    "We've spent several weeks learning about Neural Networks.\n",
    "At best, we suggested theories for how they are able to achieve what they do.\n",
    "\n",
    "This week: we will explore methods for testing theories as to how Neural Networks work.\n",
    "\n",
    "We begin by introducing a technique called *Gradient Ascent*. \n",
    "\n",
    "This will be useful both to illustrate Advanced Keras but also to introduce a tool for interpretation.\n",
    "\n",
    "**Learning to learn** \n",
    "- [The new paradigm: Pre-Trained LM + Fine-Tuning](NLP_Language_Models.ipynb#Language-Models:-the-future-(present-?)-of-NLP-?)\n",
    "- [A Universal Model/API](NLP_Universal_Model.ipynb)\n",
    "- [Beyond the LLM](NLP_Beyond_LLM.ipynb)\n",
    "\n",
    "**Generative ML**\n",
    "- [Autoencoders](Autoencoders_and_Generative_Models.ipynb)\n",
    "- [Generative Adversarial Networks](GAN_Overview.ipynb)\n",
    "- [Neural Style Transfer](Neural_Style_Transfer.ipynb)\n",
    "\n",
    "**Advanced Keras**\n",
    "- [Advanced Keras](Keras_Advanced.ipynb)\n",
    "- [Gradient Ascent](Gradient_ascent.ipynb)\n",
    "    - Useful for Interpretaton\n",
    "\n",
    "**What is a Neural Network really doing ? Interpretation**\n",
    "- [Interpretation: preview](Interpretation_preview.ipynb)\n",
    "- [Introduction to Interpretation of Deep Learning](Intro_to_Interpretation_of_DL.ipynb)\n",
    "- [Interpretation: Simple Methods](Interpretation_of_DL_Simple.ipynb)\n",
    "- [Interpretation: Saliency Maps](Interpretation_of_DL_Deconv.ipynb)\n",
    "- [Interpretation: Gradient Ascent](Interpretation_of_DL_Gradient_Ascent.ipynb)\n",
    "- [Adversarial Examples](Adversarial_Examples.ipynb)\n",
    "\n",
    "**Wrapping up**\n",
    "- [Final thoughts](Deep_Learning_Coda.ipynb)\n",
    "\n",
    "**Deeper Dives**\n",
    "- [Factor Models and Autoencoders](Autoencoder_for_conditional_risk_factors.ipynb)\n",
    "- [Interpretation using Principal Components](Interpretation_of_DL_Simple_PCA.ipynb)\n",
    "- [Interpretation: Attention](Interpretation_of_DL_Attention.ipynb)\n",
    "\n",
    "\n",
    "**Further reading**\n",
    "- [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "- [From GAN to WGAN](https://lilianweng.github.io/posts/2017-08-20-gan/)\n",
    "- [Wasserstein GAN](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html)\n",
    "- [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf)\n",
    "- [Old blog: How CNN's see the world](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\n",
    "    - **Note**: the code is obsolete, but the discussion is very good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "- Assignment notebook: [Using Machine Learning for Hedging](assignments/Regression%20task/Using_Machine_Learning_for_Hedging.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Regression task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "- Assignment notebook: [Ships in satellite images](assignments/Classification%20task/Ships_in_satellite_images.ipynb#)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Classification task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Midterm Project: Bankruptcy One Year Ahead\n",
    "- Assignment notebook [Bankruptcy One Year Ahead](assignments/bankruptcy_one_yr/Bankruptcy_oya.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Bankruptcy One Year Ahead\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras practice\n",
    "- Assignment notebook [Ships in satellite images: Neural Network](assignments/keras_intro/Ships_in_satellite_images_P1.ipynb)\n",
    "- Data (same as for the Classification assignment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "- Assignment notebook [Ships in satellite images: Neural Network](assignments/CNN_intro/Ships_in_satellite_images_P2.ipynb)\n",
    "- Data (same as for the Classification assignment)\n",
    "    - please repeat the directions given in that assignment for obtaining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final project; Stock prediction\n",
    "\n",
    " - Assignment notebooks:\n",
    "    - [Stock prediction](assignments/stock_prediction/Final_project_StockPrediction.ipynb)\n",
    "    - [Submission guidelines](assignments/stock_prediction/Final_project.ipynb)\n",
    "   \n",
    " - Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Stock Prediction\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook, submission guidelines notebook\n",
    "        - A directory named `Data/train`\n",
    "\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
